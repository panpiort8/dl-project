{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = \"master\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAirt--iS9rD"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrq7HeEFS9rQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "!git clone --single-branch --branch $branch https://github.com/panpiort8/dl-project\n",
    "%cd dl-project/\n",
    "!git checkout $branch\n",
    "!git pull\n",
    "!pip install transformers\n",
    "!pip install wandb\n",
    "%cd ..\n",
    "\n",
    "import sys\n",
    "sys.path.append('dl-project')\n",
    "sys.path.append('dl-project/universal_computation')\n",
    "\n",
    "from universal_computation.experiment import run_experiment\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymnIH8-VS9rR"
   },
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u5lmHADFWiYw"
   },
   "outputs": [],
   "source": [
    "experiment_name = 'fpt'\n",
    "\n",
    "experiment_params = dict(\n",
    "    task='bit-memory',\n",
    "    n=1000,                # ignored if not a bit task\n",
    "    num_patterns=5,        # ignored if not a bit task\n",
    "    patch_size=50,\n",
    "\n",
    "    model_name='gpt2',\n",
    "    pretrained=True,\n",
    "\n",
    "    freeze_trans=True,     # if False, we don't check arguments other than in and out\n",
    "    freeze_in=False,\n",
    "    freeze_pos=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_out=False,\n",
    "\n",
    "    in_layer_sizes=None,   # not in paper, but can specify layer sizes for an MLP,\n",
    "    out_layer_sizes=None,  # ex. [32, 32] creates a 2-layer MLP with dimension 32\n",
    "\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=2,\n",
    "    dropout=0.1,\n",
    "    orth_gain=1.41,\n",
    ")\n",
    "\n",
    "class Args:\n",
    "    num_iters = 10000         # Number of iterations for trainer\n",
    "    steps_per_iter = 100      # Number of gradient steps per iteration\n",
    "    test_steps_per_iter = 25  # Number of test gradient steps per iteration\n",
    "    log_to_wandb = False      # Whether or not to log to Weights and Biases\n",
    "    note = ''                 # An optional note to be logged to W&\n",
    "    wandb_project = ''        # Project name for W&B\n",
    "    include_date = True       # Whether to include date in run name\n",
    "    save_models = False       # Whether or not to save the model files locally\n",
    "    save_models_ever = 25     # How often to save models locally\n",
    "    device = 'cuda'           # Which device for Pytorch to use\n",
    "    gpu_batch_size = 16       # Max batch size to put on GPU (used for gradient accumulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cN8CUJpckiO-"
   },
   "outputs": [],
   "source": [
    "run_experiment(experiment_name, experiment_params, Args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kopia notatnika demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
